{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "[Tutorial: Extracting Keywords with TF-IDF and Python’s Scikit-Learn](https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X8Ysl1lKhNg) &mdash; Kavita Ganesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser, time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    " \n",
    "import re\n",
    "def pre_process(text):\n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"\",\"\",text)\n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    # remove numbers\n",
    "    text = re.sub(r'[0-9\\,$]+', '', text)\n",
    "    return text\n",
    "\n",
    "def tolist(text):\n",
    "    return text.split()\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>French daily coronavirus death toll beyond the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3 charts that show the U.S. restaurant industr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Farmer at centre of COVID-19 outbreak spent $7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Video calls connect anxious parents to hospita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Coronavirus: London key workers to star on cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141203</td>\n",
       "      <td>Chinese doctors wear hazmat suits to treat cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141204</td>\n",
       "      <td>Dow Jones falls 150 points due to US Chinese c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141205</td>\n",
       "      <td>CDC Confirms Second Case Of Coronavirus In The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141206</td>\n",
       "      <td>Queues to buy face masks in China as Wuhan cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141207</td>\n",
       "      <td>Parents 'abandon' their two children at Chines...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141208 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     News\n",
       "0       French daily coronavirus death toll beyond the...\n",
       "1       3 charts that show the U.S. restaurant industr...\n",
       "2       Farmer at centre of COVID-19 outbreak spent $7...\n",
       "3       Video calls connect anxious parents to hospita...\n",
       "4       Coronavirus: London key workers to star on cov...\n",
       "...                                                   ...\n",
       "141203  Chinese doctors wear hazmat suits to treat cor...\n",
       "141204  Dow Jones falls 150 points due to US Chinese c...\n",
       "141205  CDC Confirms Second Case Of Coronavirus In The...\n",
       "141206  Queues to buy face masks in China as Wuhan cor...\n",
       "141207  Parents 'abandon' their two children at Chines...\n",
       "\n",
       "[141208 rows x 1 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdlines_df = pd.read_csv('2020-headlines.csv') \n",
    "hdlines_df.dropna(inplace=True)\n",
    "hdlines_df.drop(columns=['SNO', 'Website'], inplace=True)\n",
    "hdlines_df\n",
    "df_idf=hdlines_df\n",
    "df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         french daily coronavirus death toll beyond the...\n",
       "1          charts that show the u s restaurant industry ...\n",
       "2         farmer at centre of covid outbreak spent to ho...\n",
       "3         video calls connect anxious parents to hospita...\n",
       "4         coronavirus london key workers to star on cove...\n",
       "                                ...                        \n",
       "141203    chinese doctors wear hazmat suits to treat cor...\n",
       "141204    dow jones falls points due to us chinese coron...\n",
       "141205    cdc confirms second case of coronavirus in the...\n",
       "141206    queues to buy face masks in china as wuhan cor...\n",
       "141207    parents abandon their two children at chinese ...\n",
       "Name: text, Length: 141208, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_idf['text'] = df_idf['News']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
    "df_idf['text']\n",
    "docs = df_idf['text']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coronavirus', 92608),\n",
       " ('covid', 13557),\n",
       " ('from', 9965),\n",
       " ('says', 9774),\n",
       " ('with', 9085),\n",
       " ('trump', 8986),\n",
       " ('lockdown', 8684),\n",
       " ('after', 8381),\n",
       " ('cases', 7467),\n",
       " ('china', 7318),\n",
       " ('pandemic', 6873),\n",
       " ('amid', 6648),\n",
       " ('over', 6409),\n",
       " ('virus', 5119),\n",
       " ('will', 4982),\n",
       " ('outbreak', 4712),\n",
       " ('death', 4610),\n",
       " ('more', 4467),\n",
       " ('during', 4276),\n",
       " ('could', 3994)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "wordsFreq = defaultdict(int)\n",
    "lines = docs.tolist()\n",
    "for line in lines:\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if len(word) > 3:\n",
    "            wordsFreq[word] += 1\n",
    "\n",
    "\n",
    "sorted(wordsFreq.items(), key=lambda k_v: k_v[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer() \n",
    "cv=CountVectorizer(max_df=0.85,stop_words='english')\n",
    "word_count_vector=cv.fit_transform(df_idf['News'])\n",
    "word_count_vector.shape\n",
    "# word_count_vector\n",
    "\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141208, 33003)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you only needs to do this once, this is a mapping of index to \n",
    "feature_names=cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaccines Are Coming, but Pandemic Experts Expect a 'Horrible' Winter\n",
      "vaccines are coming but pandemic experts expect a horrible winter\n",
      "\n",
      "=====Doc=====\n",
      "['vaccines', 'are', 'coming', 'but', 'pandemic', 'experts', 'expect', 'a', 'horrible', 'winter']\n",
      "\n",
      "===Keywords===\n",
      "winter 1.0\n",
      "vaccines 1.0\n",
      "pandemic 1.0\n",
      "horrible 1.0\n",
      "experts 1.0\n",
      "Pandemic-Proof Your Habits\n",
      "pandemic proof your habits\n",
      "\n",
      "=====Doc=====\n",
      "['pandemic', 'proof', 'your', 'habits']\n",
      "\n",
      "===Keywords===\n",
      "proof 1.0\n",
      "pandemic 1.0\n",
      "habits 1.0\n",
      "Vaccines Are Coming, but Pandemic Experts Expect a 'Horrible' Winter\n",
      "vaccines are coming but pandemic experts expect a horrible winter\n",
      "\n",
      "=====Doc=====\n",
      "['vaccines', 'are', 'coming', 'but', 'pandemic', 'experts', 'expect', 'a', 'horrible', 'winter']\n",
      "\n",
      "===Keywords===\n",
      "winter 1.0\n",
      "vaccines 1.0\n",
      "pandemic 1.0\n",
      "horrible 1.0\n",
      "experts 1.0\n",
      "‘Bleak Friday’ for Stores as Pandemic Pushes Holiday Shopping Online\n",
      " bleak friday for stores as pandemic pushes holiday shopping online\n",
      "\n",
      "=====Doc=====\n",
      "['bleak', 'friday', 'for', 'stores', 'as', 'pandemic', 'pushes', 'holiday', 'shopping', 'online']\n",
      "\n",
      "===Keywords===\n",
      "stores 1.0\n",
      "shopping 1.0\n",
      "pushes 1.0\n",
      "pandemic 1.0\n",
      "online 1.0\n",
      "Bitcoin Climbs to Record High\n",
      "bitcoin climbs to record high\n",
      "\n",
      "=====Doc=====\n",
      "['bitcoin', 'climbs', 'to', 'record', 'high']\n",
      "\n",
      "===Keywords===\n",
      "record 1.0\n",
      "high 1.0\n",
      "climbs 1.0\n",
      "bitcoin 1.0\n",
      "NFL Week 12: What We Learned\n",
      "nfl week what we learned\n",
      "\n",
      "=====Doc=====\n",
      "['nfl', 'week', 'what', 'we', 'learned']\n",
      "\n",
      "===Keywords===\n",
      "week 1.0\n",
      "nfl 1.0\n",
      "learned 1.0\n",
      "London A.I. Lab Claims Breakthrough That Could Accelerate Drug Discovery\n",
      "london a i lab claims breakthrough that could accelerate drug discovery\n",
      "\n",
      "=====Doc=====\n",
      "['london', 'a', 'i', 'lab', 'claims', 'breakthrough', 'that', 'could', 'accelerate', 'drug', 'discovery']\n",
      "\n",
      "===Keywords===\n",
      "london 1.0\n",
      "lab 1.0\n",
      "drug 1.0\n",
      "discovery 1.0\n",
      "claims 1.0\n",
      "Noah Hawley Isn’t Done with ‘Fargo’\n",
      "noah hawley isn t done with fargo \n",
      "\n",
      "=====Doc=====\n",
      "['noah', 'hawley', 'isn', 't', 'done', 'with', 'fargo']\n",
      "\n",
      "===Keywords===\n",
      "noah 1.0\n",
      "isn 1.0\n",
      "hawley 1.0\n",
      "fargo 1.0\n",
      "Hidden in Plain Sight: The Ghosts of Segregation\n",
      "hidden in plain sight the ghosts of segregation\n",
      "\n",
      "=====Doc=====\n",
      "['hidden', 'in', 'plain', 'sight', 'the', 'ghosts', 'of', 'segregation']\n",
      "\n",
      "===Keywords===\n",
      "sight 1.0\n",
      "segregation 1.0\n",
      "plain 1.0\n",
      "hidden 1.0\n",
      "ghosts 1.0\n",
      "What to Know About the Supreme Court Census Case\n",
      "what to know about the supreme court census case\n",
      "\n",
      "=====Doc=====\n",
      "['what', 'to', 'know', 'about', 'the', 'supreme', 'court', 'census', 'case']\n",
      "\n",
      "===Keywords===\n",
      "supreme 1.0\n",
      "know 1.0\n",
      "court 1.0\n",
      "census 1.0\n",
      "case 1.0\n",
      "Covid infections in England fall by 30% over lockdown - React study\n",
      "covid infections in england fall by over lockdown react study\n",
      "\n",
      "=====Doc=====\n",
      "['covid', 'infections', 'in', 'england', 'fall', 'by', 'over', 'lockdown', 'react', 'study']\n",
      "\n",
      "===Keywords===\n",
      "study 1.0\n",
      "react 1.0\n",
      "lockdown 1.0\n",
      "infections 1.0\n",
      "fall 1.0\n",
      "Gov Newsom says projections show CA will run out of current ICU beds before Christmas Eve\n",
      "gov newsom says projections show ca will run out of current icu beds before christmas eve\n",
      "\n",
      "=====Doc=====\n",
      "['gov', 'newsom', 'says', 'projections', 'show', 'ca', 'will', 'run', 'out', 'of', 'current', 'icu', 'beds', 'before', 'christmas', 'eve']\n",
      "\n",
      "===Keywords===\n",
      "says 1.0\n",
      "run 1.0\n",
      "projections 1.0\n",
      "newsom 1.0\n",
      "icu 1.0\n",
      "Topshop owner Arcadia goes into administration\n",
      "topshop owner arcadia goes into administration\n",
      "\n",
      "=====Doc=====\n",
      "['topshop', 'owner', 'arcadia', 'goes', 'into', 'administration']\n",
      "\n",
      "===Keywords===\n",
      "topshop 1.0\n",
      "owner 1.0\n",
      "goes 1.0\n",
      "arcadia 1.0\n",
      "administration 1.0\n",
      "Coronavirus: Government publishes data behind stricter tiers\n",
      "coronavirus government publishes data behind stricter tiers\n",
      "\n",
      "=====Doc=====\n",
      "['coronavirus', 'government', 'publishes', 'data', 'behind', 'stricter', 'tiers']\n",
      "\n",
      "===Keywords===\n",
      "tiers 1.0\n",
      "stricter 1.0\n",
      "publishes 1.0\n",
      "government 1.0\n",
      "data 1.0\n",
      "Covid: How might GCSE and A-levels work this summer?\n",
      "covid how might gcse and a levels work this summer \n",
      "\n",
      "=====Doc=====\n",
      "['covid', 'how', 'might', 'gcse', 'and', 'a', 'levels', 'work', 'this', 'summer']\n",
      "\n",
      "===Keywords===\n",
      "work 1.0\n",
      "summer 1.0\n",
      "levels 1.0\n",
      "gcse 1.0\n",
      "covid 1.0\n",
      "One of biology's biggest mysteries 'largely solved' by AI\n",
      "one of biology s biggest mysteries largely solved by ai\n",
      "\n",
      "=====Doc=====\n",
      "['one', 'of', 'biology', 's', 'biggest', 'mysteries', 'largely', 'solved', 'by', 'ai']\n",
      "\n",
      "===Keywords===\n",
      "solved 1.0\n",
      "mysteries 1.0\n",
      "largely 1.0\n",
      "biology 1.0\n",
      "biggest 1.0\n",
      "Facebook News will pay UK outlets for content in 2021\n",
      "facebook news will pay uk outlets for content in \n",
      "\n",
      "=====Doc=====\n",
      "['facebook', 'news', 'will', 'pay', 'uk', 'outlets', 'for', 'content', 'in']\n",
      "\n",
      "===Keywords===\n",
      "uk 1.0\n",
      "pay 1.0\n",
      "outlets 1.0\n",
      "news 1.0\n",
      "facebook 1.0\n",
      "Doctor Who: Bradley Walsh and Tosin Cole to leave companion roles\n",
      "doctor who bradley walsh and tosin cole to leave companion roles\n",
      "\n",
      "=====Doc=====\n",
      "['doctor', 'who', 'bradley', 'walsh', 'and', 'tosin', 'cole', 'to', 'leave', 'companion', 'roles']\n",
      "\n",
      "===Keywords===\n",
      "walsh 1.0\n",
      "roles 1.0\n",
      "leave 1.0\n",
      "doctor 1.0\n",
      "companion 1.0\n",
      "Saudi Arabia allows Israeli commercial planes to use its airspace\n",
      "saudi arabia allows israeli commercial planes to use its airspace\n",
      "\n",
      "=====Doc=====\n",
      "['saudi', 'arabia', 'allows', 'israeli', 'commercial', 'planes', 'to', 'use', 'its', 'airspace']\n",
      "\n",
      "===Keywords===\n",
      "use 1.0\n",
      "saudi 1.0\n",
      "planes 1.0\n",
      "israeli 1.0\n",
      "commercial 1.0\n",
      "Saudi Arabia allows Israeli commercial planes to use its airspace\n",
      "saudi arabia allows israeli commercial planes to use its airspace\n",
      "\n",
      "=====Doc=====\n",
      "['saudi', 'arabia', 'allows', 'israeli', 'commercial', 'planes', 'to', 'use', 'its', 'airspace']\n",
      "\n",
      "===Keywords===\n",
      "use 1.0\n",
      "saudi 1.0\n",
      "planes 1.0\n",
      "israeli 1.0\n",
      "commercial 1.0\n"
     ]
    }
   ],
   "source": [
    "class Feed:\n",
    "    name = ''\n",
    "    url = None\n",
    "    max_delay = None\n",
    "    def __init__(self, name, url, max_delay):\n",
    "        self.name = name\n",
    "        self.url  = url\n",
    "        self.max_delay = max_delay\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '%s: %s' % (self.name, self.url)\n",
    "\n",
    "    def getHeadline(self):\n",
    "        d = feedparser.parse (self.url)\n",
    "        for post in d.entries:\n",
    "            # time.sleep(0.125)\n",
    "            ret = (datetime.datetime.now().time(), self.name, post.title, post.link)\n",
    "            yield ret\n",
    "\n",
    "#\n",
    "\n",
    "feeds = (\n",
    "    Feed('nyt-home', 'https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml', 1),\n",
    "    Feed('nyt-sun', 'https://rss.nytimes.com/services/xml/rss/nyt/sunday-review.xml', 1),\n",
    "    Feed('nyt-hlth', 'https://rss.nytimes.com/services/xml/rss/nyt/Health.xml', 1),\n",
    "    Feed('nyt-wrld', 'https://www.nytimes.com/section/world/rss.xml', 1),\n",
    "    Feed('nyt-bsns', 'http://feeds.nytimes.com/nyt/rss/Business', 1),\n",
    "    Feed('nyt-tech', 'http://feeds.nytimes.com/nyt/rss/Technology', 1),\n",
    "    Feed('nyt-sprt', 'https://rss.nytimes.com/services/xml/rss/nyt/Sports.xml', 1),\n",
    "    Feed('nyt-scnc', 'http://www.nytimes.com/services/xml/rss/nyt/Science.xml', 1),\n",
    "    Feed('nyt-arts', 'https://rss.nytimes.com/services/xml/rss/nyt/Arts.xml', 1),\n",
    "    Feed('nyt-trvl', 'https://rss.nytimes.com/services/xml/rss/nyt/Travel.xml', 1),\n",
    "    Feed('nyt-usa',  'http://www.nytimes.com/services/xml/rss/nyt/US.xml', 1),\n",
    "    Feed('bbc-hlth', 'http://feeds.bbci.co.uk/news/health/rss.xml', 1),\n",
    "    Feed('bbc-brkn', 'https://bbcbreakingnews.com/feed', 1),\n",
    "    Feed('bbc-bsns', 'http://feeds.bbci.co.uk/news/business/rss.xml', 1),\n",
    "    Feed('bbc-pltc', 'http://feeds.bbci.co.uk/news/politics/rss.xml', 1),\n",
    "    Feed('bbc-educ', 'http://feeds.bbci.co.uk/news/education/rss.xml', 1),\n",
    "    Feed('bbc-scnc', 'http://feeds.bbci.co.uk/news/science_and_environment/rss.xml', 1),\n",
    "    Feed('bbc-tech', 'http://feeds.bbci.co.uk/news/technology/rss.xml', 1),\n",
    "    Feed('bbc-arts', 'http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml', 1),\n",
    "    Feed('aljz-ra', 'http://www.aljazeera.com/xml/rss/all.xml', 1),\n",
    "    Feed('aljz-ra', 'http://www.aljazeera.com/xml/rss/all.xml', 1),\n",
    ")\n",
    "\n",
    "titles = []\n",
    "for feed in feeds:\n",
    "    for (tt, name, title, link) in feed.getHeadline():\n",
    "        print(title)\n",
    "        print(pre_process(title))\n",
    "        test_doc = pre_process(title)\n",
    "        doc = tolist(test_doc)\n",
    "        #generate tf-idf for the given document\n",
    "        tf_idf_vector = tfidf_transformer.transform(cv.transform(doc))\n",
    "        \n",
    "        #sort the tf-idf vectors by descending order of scores\n",
    "        sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "        #extract only the top n; n here is 10\n",
    "        keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "\n",
    "        # now print the results\n",
    "        print(\"\\n=====Doc=====\")\n",
    "        print(doc)\n",
    "        print(\"\\n===Keywords===\")\n",
    "        for k in keywords:\n",
    "            print(k,keywords[k])\n",
    "        \n",
    "        # titles.append(title)\n",
    "        # print (tt, name, title, link)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test=df_test['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfTransformer' object has no attribute '_idf_diag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-cce3b4a9e9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print idf values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idf_weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# sort ascending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# df_idf2 = df_idf[df_idf['idf_weights'] > 3.5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/comp119/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36midf_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# if _idf_diag is not set, this will raise an attribute error,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m# which means hasattr(self, \"idf_\") is False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_idf_diag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0midf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TfidfTransformer' object has no attribute '_idf_diag'"
     ]
    }
   ],
   "source": [
    "# print idf values \n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(), columns=[\"idf_weights\"]) \n",
    " \n",
    "# sort ascending \n",
    "# df_idf2 = df_idf[df_idf['idf_weights'] > 3.5]\n",
    "df_idf2.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33283"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count matrix \n",
    "count_vector=cv.transform(docs) \n",
    " \n",
    "# tf-idf scores \n",
    "tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french daily coronavirus death toll beyond the mark\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>saudi</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tfidf\n",
       "saudi    1.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "feature_names = cv.get_feature_names() \n",
    " \n",
    "#get tfidf vector for first document \n",
    "first_document_vector=tf_idf_vector[n] \n",
    "print (docs[n])\n",
    "#print the scores \n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df2 = df[df['tfidf'] > 0]\n",
    "df2.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    " \n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    " \n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
